{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d924c2af",
   "metadata": {},
   "source": [
    "### Imports and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69f538d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#Properties\n",
    "from utils import properties as p\n",
    "\n",
    "#Utils\n",
    "from utils import create_bins, one_hot, print_summary\n",
    "\n",
    "#Model\n",
    "import nn_model as nn\n",
    "\n",
    "#Config\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb45084e",
   "metadata": {},
   "source": [
    "### Setup properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cd83d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.update({\n",
    "    \n",
    "    #Age range\n",
    "    'age_min' : 0,\n",
    "    'age_max' : 49,\n",
    "    \n",
    "    #Number of data samples\n",
    "    'data_samples' : 3000,\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4791142",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17e129e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../dataset/data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b234f7",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "481b5ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_filter(df, age_min, age_max):\n",
    "    return (df['age'] >= age_min) & (df['age'] <= age_max)\n",
    "\n",
    "def ethnicity_gender_filter(df, i, j):\n",
    "    return (df['ethnicity'] == p['ETHNICITIES'][i]) & (df['gender'] == p['GENDERS'][j])\n",
    "\n",
    "def ethnicity_age_filter(df, e, age_min, age_max):\n",
    "    return (df['ethnicity'] == p['ETHNICITIES'][e]) & age_filter(df, age_min, age_max)\n",
    "\n",
    "def ethnicity_gender_age_filter(df, i, j, age_min, age_max):\n",
    "    return ethnicity_gender_filter(df, i, j) & age_filter(df, age_min, age_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "330dfba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[age_filter(df, p['age_min'], p['age_max'])]\n",
    "df['ethnicity'] = df['ethnicity'].map(p['ETHNICITIES'])\n",
    "df['gender'] = df['gender'].map(p['GENDERS'])\n",
    "df[p['feature_to_use']] = df[p['feature_to_use']]\\\n",
    "        .apply(lambda x: np.array(x.split(), dtype='float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740987e5",
   "metadata": {},
   "source": [
    "### Create balanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64e807db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ethnicity gender age sample size\n",
    "ethnicity_age_sample_size = round(p['data_samples'] / (round(p['age_range']() / p['age_bins']) * len(p['ETHNICITIES'])))\n",
    "\n",
    "balanced_df = pd.concat([\n",
    "    df.loc[ethnicity_age_filter(df, e, age_m, age_M)]\\\n",
    "        .sample(ethnicity_age_sample_size, random_state=p['seed'], replace=True)\n",
    "    for e in range(len(p['ETHNICITIES']))\n",
    "    #for j in range(len(GENDERS))\n",
    "    for (age_m, age_M) in p['bins']()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f40c0a3",
   "metadata": {},
   "source": [
    "### Create unbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51b6ee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_factor = 0.05 # 5% of the population \n",
    "maj_factor = 1 - min_factor\n",
    "\n",
    "#Size of minority group\n",
    "min_sample_size = round(p['data_samples'] * min_factor / len(p['bins']()) / (len(p['ETHNICITIES']) - 1))\n",
    "\n",
    "#Size of majority group\n",
    "maj_sample_size = round(p['data_samples'] * maj_factor / len(p['GENDERS']) / len(p['bins']()))\n",
    "\n",
    "white_male = pd.concat([\n",
    "    df.loc[ethnicity_gender_age_filter(df, 0, 0, age_min, age_max)]\\\n",
    "        .sample(maj_sample_size, random_state=p['seed'], replace=True)\n",
    "    for (age_min, age_max) in p['bins']()\n",
    "])\n",
    "\n",
    "white_female = pd.concat([\n",
    "    df.loc[ethnicity_gender_age_filter(df, 0, 1, age_min, age_max)]\\\n",
    "        .sample(maj_sample_size, random_state=p['seed'], replace=True)\n",
    "    for (age_min, age_max) in p['bins']()\n",
    "])\n",
    "\n",
    "unbalanced_df = pd.concat([\n",
    "    df.loc[ethnicity_age_filter(df, e, age_min, age_max)]\\\n",
    "        .sample(min_sample_size, random_state=p['seed'], replace=True)\n",
    "    for e in range(1, len(p['ETHNICITIES']))\n",
    "    #for j in range(len(GENDERS))\n",
    "    for (age_min, age_max) in p['bins']()\n",
    "] + [white_female, white_male])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e83e3da",
   "metadata": {},
   "source": [
    "### Create Train-Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be47655c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test(df, test_size=.2):\n",
    "    return train_test_split(df, test_size=test_size, random_state=p['seed'])\n",
    "\n",
    "def prepare_X(df):\n",
    "    X = np.array(df[p['feature_to_use']].tolist())\n",
    "    X = np.reshape(X, (-1, 1, 1, p['img_size'] * p['img_size'])) / p['color_channel']\n",
    "    return X\n",
    "\n",
    "def prepare_y(df):\n",
    "    y = np.array(df[p['feature_to_predict']].apply(lambda x: one_hot(x, p['bins']())).tolist())\n",
    "    y = np.reshape(y, (-1, 1, 1, len(p['bins']())))\n",
    "    return y\n",
    "    \n",
    "def prepare_data(df_train, df_test):\n",
    "    X_train = prepare_X(df_train)\n",
    "    y_train = prepare_y(df_train)\n",
    "    X_test = prepare_X(df_test)\n",
    "    y_test = prepare_y(df_test)\n",
    "    return X_train, y_train, X_test, y_test\n",
    "    \n",
    "\n",
    "#df_train, df_test = create_train_test(balanced_df)\n",
    "#X_train, y_train, X_test, y_test = prepare_data(df_train, df_test)\n",
    "#X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a907e00f",
   "metadata": {},
   "source": [
    "## Predictive Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b678f05",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a06b5d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.05\n",
    "epochs_n = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ccb695",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdbd2dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# network (not federated)\n",
    "\n",
    "def standard_nn():\n",
    "    return nn.get_standard_nn(p['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852aa5ed",
   "metadata": {},
   "source": [
    "### Evaluate predictive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98b1a550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ethnicity(dataset, ethnicity, df_test, model):\n",
    "    idx_keys = dataset.index.intersection(df_test[df_test['ethnicity'] == ethnicity].index)\n",
    "    ethn_df = dataset.loc[idx_keys]\n",
    "    ethn_X = prepare_X(ethn_df)\n",
    "    ethn_y = prepare_y(ethn_df)\n",
    "    ethn_y_pred = model.predict(ethn_X)\n",
    "    score = f1_score(convert_output(ethn_y), convert_output(ethn_y_pred), average='macro')\n",
    "    return score\n",
    "\n",
    "def evaluate_all_ethnicities(dataset, df_test, model):\n",
    "    scores = [evaluate_ethnicity(dataset, e, df_test, model) for e in p['ETHNICITIES'].values()]\n",
    "    return dict(zip(p['ETHNICITIES'].values(), scores))\n",
    "\n",
    "def one_hot_to_num(oh):\n",
    "    M = max(oh)\n",
    "    return oh.index(M)\n",
    "\n",
    "def convert_output(y_out):\n",
    "    out = []\n",
    "    for i in range(len(y_out)):\n",
    "        label = one_hot_to_num(list(y_out[i][0][0]))\n",
    "        out += [one_hot(label * p['age_bins'], p['bins']())]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8599b8",
   "metadata": {},
   "source": [
    "### F1 Score\n",
    "#### Params \n",
    "`macro`: Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account. <br><br>\n",
    "`weighted`: Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52b8350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_evaluate(df, model, exp_id):\n",
    "    df_train, df_test = create_train_test(df)\n",
    "    X_train, y_train, X_test, y_test = prepare_data(df_train, df_test)\n",
    "    for i in range(len(X_train)):\n",
    "        model.fit(X_train[i], y_train[i], epochs=epochs_n, learning_rate=learning_rate)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    ev = dict()\n",
    "    ev['experiment ID'] = exp_id\n",
    "    ev['data sample #'] = round(len(df))\n",
    "    ev['f1 [macro]'] = f1_score(convert_output(y_test), convert_output(y_pred), average='macro')\n",
    "    ev.update(evaluate_all_ethnicities(df, df_test, model))\n",
    "    \n",
    "    print_summary(df_train, exp_id + ' [train]')\n",
    "    print_summary(df_test, exp_id + ' [test]')\n",
    "    return ev\n",
    "\n",
    "def compare_model_behaviour(df_list, model_list, exp_ids):\n",
    "    return [predict_and_evaluate(df_list[i], model_list[i], exp_ids[i]) for i in range(len(df_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70317431",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    }
   ],
   "source": [
    "metrics = compare_model_behaviour(\n",
    "    [balanced_df, unbalanced_df, df.sample(p['data_samples'], random_state=p['seed'])], \n",
    "    [standard_nn()] * 3,\n",
    "    exp_ids=['Balanced dataset', 'Unbalanced dataset', 'Dataset']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e1ab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_data(metrics):\n",
    "    metrics_df = pd.DataFrame()\n",
    "    for m in metrics:\n",
    "        row = pd.Series(m)\n",
    "        metrics_df = metrics_df.append(row, ignore_index=True)\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023cac44",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics_data(metrics)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
